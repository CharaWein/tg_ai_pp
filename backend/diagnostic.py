# diagnostic.py - ะะะะะะะกะขะะะ ะะะะะะะะซ ะก EMPTY ANSWERS
# ะกะบัะธะฟั ะดะปั ะฟัะพะฒะตัะบะธ ะบะฐะถะดะพะณะพ ะบะพะผะฟะพะฝะตะฝัะฐ ัะธััะตะผั

import os
import sys
import json
import requests
from pathlib import Path

print("\n" + "=" * 70)
print("๐ ะะะะะะะกะขะะะ RAG AI-ะะะะะ")
print("=" * 70 + "\n")

# ========== ะจะะะ ะะะะะะะกะขะะะ ==========

# ะจะะะ 1: ะัะพะฒะตัะบะฐ config.py
print("[1๏ธโฃ] ะัะพะฒะตัะบะฐ CONFIG.PY")
print("-" * 70)
try:
    from config import BOT_TOKEN, OLLAMA_API_URL, OLLAMA_MODEL, DEBUG
    print(f"โ Config ะทะฐะณััะถะตะฝ")
    print(f"   BOT_TOKEN: {'***' if BOT_TOKEN else 'โ ะะ ะฃะกะขะะะะะะะ'}")
    print(f"   OLLAMA_API_URL: {OLLAMA_API_URL}")
    print(f"   OLLAMA_MODEL: {OLLAMA_MODEL}")
    print(f"   DEBUG: {DEBUG}")
except Exception as e:
    print(f"โ ะัะธะฑะบะฐ ะทะฐะณััะทะบะธ config: {e}")
    sys.exit(1)

# ะจะะะ 2: ะัะพะฒะตัะบะฐ OLLAMA ะฟะพะดะบะปััะตะฝะธั
print("\n[2๏ธโฃ] ะัะพะฒะตัะบะฐ OLLAMA ะะะะะะฎะงะะะะฏ")
print("-" * 70)
try:
    resp = requests.get(f"{OLLAMA_API_URL}/api/tags", timeout=5)
    if resp.status_code == 200:
        models = resp.json().get('models', [])
        print(f"โ OLLAMA ะทะฐะฟััะตะฝะฐ!")
        print(f"   ะฃััะฐะฝะพะฒะปะตะฝะฝัะต ะผะพะดะตะปะธ:")
        for model in models:
            model_name = model.get('name', 'Unknown')
            print(f"   - {model_name}")
        
        # ะัะพะฒะตััะตะผ ะฝัะถะฝัั ะผะพะดะตะปั
        if any(OLLAMA_MODEL in m.get('name', '') for m in models):
            print(f"   โ ะะพะดะตะปั {OLLAMA_MODEL} ะฝะฐะนะดะตะฝะฐ!")
        else:
            print(f"   โ ะะพะดะตะปั {OLLAMA_MODEL} ะะ ะะะะะะะ!")
            print(f"      ะะฐะฟัััะธ: ollama pull {OLLAMA_MODEL.split(':')[0]}")
    else:
        print(f"โ OLLAMA ะพัะฒะตัะธะปะฐ ั ะบะพะดะพะผ {resp.status_code}")
except requests.ConnectionError:
    print(f"โ ะะ ะะะะฃ ะะะะะะฎะงะะขะฌะกะฏ ะ OLLAMA")
    print(f"   ะฃะฑะตะดะธัั ััะพ OLLAMA ะทะฐะฟััะตะฝะฐ: ollama serve")
    print(f"   URL: {OLLAMA_API_URL}")
except Exception as e:
    print(f"โ ะัะธะฑะบะฐ ะฟัะพะฒะตัะบะธ OLLAMA: {e}")

# ะจะะะ 3: ะัะพะฒะตัะบะฐ ัะฐะนะปะพะฒ ะดะฐะฝะฝัั
print("\n[3๏ธโฃ] ะัะพะฒะตัะบะฐ ะคะะะะะ ะะะะะซะฅ")
print("-" * 70)
files_to_check = [
    'data/user_messages.json',
    'data/facts_advanced.json',
    'data/prompt_template.json',
    'data/dialogue_history.json'
]

for file_path in files_to_check:
    if os.path.exists(file_path):
        size = os.path.getsize(file_path)
        print(f"โ {file_path} ({size} ะฑะฐะนั)")
    else:
        print(f"โ {file_path} ะะ ะะะะะะ")

# ะจะะะ 4: ะัะพะฒะตัะบะฐ prompt_template
print("\n[4๏ธโฃ] ะัะพะฒะตัะบะฐ PROMPT TEMPLATE")
print("-" * 70)
try:
    with open('data/prompt_template.json', 'r', encoding='utf-8') as f:
        template = json.load(f)
    
    system_prompt = template.get('system_prompt', '')
    if system_prompt:
        print(f"โ Prompt ะทะฐะณััะถะตะฝ ({len(system_prompt)} ัะธะผะฒะพะปะพะฒ)")
        print(f"   ะะตัะฒัะต 200 ัะธะผะฒะพะปะพะฒ:")
        print(f"   {system_prompt[:200]}...")
    else:
        print(f"โ Prompt ะฟัััะพะน!")
except Exception as e:
    print(f"โ ะัะธะฑะบะฐ ะทะฐะณััะทะบะธ prompt: {e}")

# ะจะะะ 5: ะขะตััะพะฒัะน ะทะฐะฟัะพั ะบ OLLAMA
print("\n[5๏ธโฃ] ะขะะกะขะะะซะ ะะะะะะก ะ OLLAMA")
print("-" * 70)
try:
    test_data = {
        "model": OLLAMA_MODEL,
        "messages": [
            {
                "role": "system",
                "content": "ะขั ะะฝะดัะตะน. ะัะฒะตัะฐะน ะบัะฐัะบะพ."
            },
            {
                "role": "user",
                "content": "ะัะธะฒะตั"
            }
        ],
        "stream": False,
        "temperature": 0.7
    }
    
    print(f"ะัะฟัะฐะฒะปัั ะทะฐะฟัะพั ะฝะฐ {OLLAMA_API_URL}/api/chat...")
    resp = requests.post(
        f"{OLLAMA_API_URL}/api/chat",
        json=test_data,
        timeout=30
    )
    
    if resp.status_code == 200:
        result = resp.json()
        answer = result.get('message', {}).get('content', '')
        if answer:
            print(f"โ OLLAMA ะพัะฒะตัะธะปะฐ!")
            print(f"   ะัะฒะตั: {answer[:100]}...")
        else:
            print(f"โ OLLAMA ะฒะตัะฝัะปะฐ ะฟัััะพะน ะพัะฒะตั")
            print(f"   ะะพะปะฝัะน ะพัะฒะตั: {result}")
    else:
        print(f"โ OLLAMA ะพัะธะฑะบะฐ {resp.status_code}")
        print(f"   {resp.text[:200]}")
except Exception as e:
    print(f"โ ะัะธะฑะบะฐ ะฟัะธ ัะตััะพะฒะพะผ ะทะฐะฟัะพัะต: {e}")

# ะจะะะ 6: ะัะพะฒะตัะบะฐ llm_generator_final.py
print("\n[6๏ธโฃ] ะัะพะฒะตัะบะฐ LLM GENERATOR")
print("-" * 70)
try:
    from llm_generator_final import generator
    print(f"โ llm_generator_final ะทะฐะณััะถะตะฝ")
    
    # ะัะพะฒะตััะตะผ ะตััั ะปะธ ChromaDB collection
    if generator.collection:
        print(f"โ ChromaDB collection ะฝะฐะนะดะตะฝะฐ")
        count = generator.collection.count()
        print(f"   ะะพะบัะผะตะฝัะพะฒ ะฒ ะฑะฐะทะต: {count}")
    else:
        print(f"โ๏ธ ChromaDB collection ะฝะต ะฝะฐะนะดะตะฝะฐ")
    
    # ะัะพะฒะตััะตะผ prompt_template
    if generator.prompt_template:
        print(f"โ Prompt template ะทะฐะณััะถะตะฝ")
    else:
        print(f"โ Prompt template ะะ ะะะะะฃะะะ")
except Exception as e:
    print(f"โ ะัะธะฑะบะฐ ะทะฐะณััะทะบะธ generator: {e}")

# ะจะะะ 7: ะัะพะฒะตัะบะฐ clean_answer ััะฝะบัะธะธ
print("\n[7๏ธโฃ] ะะะะะะะะ CLEAN_ANSWER ะคะะะฌะขะะะฆะะ")
print("-" * 70)
try:
    test_answers = [
        "ะัะธะฒะตั! ะะฐะบ ะดะตะปะฐ?",
        "ะฏ ะฝะต ัะตะปะพะฒะตะบ, ั ะฐััะธััะตะฝั",
        "ะัะธะฒะตั",
        "ะะพัะผ, ะฒัั ะบะพะดั",
        "ั ะฟะพะผะพัะฝะธะบ ai"
    ]
    
    for test_answer in test_answers:
        cleaned = generator.clean_answer(test_answer)
        status = "โ ะะะะจะะ" if cleaned else "โ ะะขะคะะะฌะขะะะะะ"
        print(f"{status}: '{test_answer}' -> '{cleaned}'")
except Exception as e:
    print(f"โ ะัะธะฑะบะฐ ะฟัะธ ัะตััะธัะพะฒะฐะฝะธะธ clean_answer: {e}")

# ะจะะะ 8: ะัะพะณะพะฒัะน ัะตะทัะปััะฐั
print("\n" + "=" * 70)
print("๐ ะะขะะะะะซะ ะะะะฃะะฌะขะะข")
print("=" * 70 + "\n")

print("""
ะะกะะ ะะกะ โ - ะะะะะะะะ ะ OLLAMA ะะขะะะขะ ะธะปะธ CLEAN_ANSWER

ะะะะะะะะซะ ะะะจะะะะฏ:

1๏ธโฃ ะะกะะ OLLAMA ะะ ะะะะฃะฉะะะ:
   ollama serve

2๏ธโฃ ะะกะะ ะะะะะะฌ ะะ ะฃะกะขะะะะะะะะ:
   ollama pull dolphin-mixtral

3๏ธโฃ ะะกะะ ะะขะะะขะซ ะคะะะฌะขะะฃะฎะขะกะฏ:
   ะัะพะฒะตัั bad_patterns ะฒ llm_generator_final.py
   ะะพะถะตั ะฑััั ัะปะธัะบะพะผ ัััะพะณะฐั ัะธะปัััะฐัะธั

4๏ธโฃ ะะกะะ ะะกะ ะะขะะะขะซ ะะฃะกะขะซะ:
   ะะพะฑะฐะฒั DEBUG ะปะพะณะธัะพะฒะฐะฝะธะต ะฒ generate_answer()
   ะะพัะผะพััะธ ะบะฐะบะพะน ะพัะฒะตั ะฟัะธัะพะดะธั ะพั OLLAMA ะดะพ ัะธะปัััะฐัะธะธ

โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
""")
