# optimized_trainer.py
import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoTokenizer, GPT2LMHeadModel
from peft import LoraConfig, get_peft_model, TaskType
from torch.utils.data import Dataset, DataLoader
import json
import os
import logging
from typing import List, Dict
import time
import re
from datetime import datetime
import numpy as np
from collections import Counter

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)



class UltraOptimizedDataset(Dataset):
    def __init__(self, texts, tokenizer, max_length=128):
        self.tokenizer = tokenizer
        self.texts = texts
        self.max_length = max_length
        
    def __len__(self):
        return len(self.texts)
        
    def __getitem__(self, idx):
        text = self.texts[idx]
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': encoding['input_ids'].flatten()
        }

class UltraOptimizedTrainer:
    def __init__(self, model_name: str = "sberbank-ai/rugpt3small_based_on_gpt2"):
        self.model_name = model_name
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"üöÄ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        self.model = None
        self.tokenizer = None
        
    def load_model(self, model_path: str = None):
        """–°–≤–µ—Ä—Ö–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Å LoRA"""
        try:
            logger.info("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å LoRA...")
            start_time = time.time()
            
            if model_path and os.path.exists(model_path):
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_path,
                    torch_dtype=torch.float32,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º float32 –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                    device_map="auto"
                )
                self.tokenizer = AutoTokenizer.from_pretrained(model_path)
            else:
                # –ó–∞–≥—Ä—É–∑–∫–∞ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω–æ–π –º–æ–¥–µ–ª–∏
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.float32,  # float32 –≤–º–µ—Å—Ç–æ float16 –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                    low_cpu_mem_usage=True,
                )
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
                
            # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è LoRA –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
            lora_config = LoraConfig(
                task_type=TaskType.CAUSAL_LM,
                inference_mode=False,
                r=16,  # –£–≤–µ–ª–∏—á–∏–º —Ä–∞–Ω–≥
                lora_alpha=32,  # –£–≤–µ–ª–∏—á–∏–º alpha
                lora_dropout=0.1,
                target_modules=["c_attn", "c_proj", "c_fc", "wte", "wpe"],  # –î–æ–±–∞–≤–∏–º embedding —Å–ª–æ–∏
                bias="none"
            )
            
            self.model = get_peft_model(self.model, lora_config)
            self.model.to(self.device)
            
            # –í–∫–ª—é—á–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è –≤—Å–µ—Ö –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            self.model.train()
            for param in self.model.parameters():
                if param.requires_grad:
                    param.requires_grad = True
            
            load_time = time.time() - start_time
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {load_time:.1f}—Å")
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            total_params = sum(p.numel() for p in self.model.parameters())
            logger.info(f"üìä –û–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {trainable_params:,} –∏–∑ {total_params:,} ({trainable_params/total_params*100:.2f}%)")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            raise

    def create_smart_prompts(self, text: str) -> List[str]:
        """–£–º–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        if not text or len(text.strip()) < 5:
            return []
            
        text = text.strip()
        prompts = []
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Å–æ–æ–±—â–µ–Ω–∏—è –∏ —Å–æ–∑–¥–∞–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –ø—Ä–æ–º–ø—Ç—ã
        if any(mark in text for mark in ['?', '—á—Ç–æ', '–∫–∞–∫', '–ø–æ—á–µ–º—É', '–∫–æ–≥–¥–∞']):
            prompts.extend([
                f"–í–æ–ø—Ä–æ—Å: {text}\n–û—Ç–≤–µ—Ç:",
                f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {text}\n–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:",
                f"–ß–µ–ª–æ–≤–µ–∫: {text}\nAI:"
            ])
        elif len(text.split()) > 8:
            prompts.extend([
                f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {text}\n–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ:",
                f"–°–æ–æ–±—â–µ–Ω–∏–µ: {text}\n–û—Ç–≤–µ—Ç:"
            ])
        else:
            prompts.extend([
                f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {text}\n–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:",
                f"–ß–µ–ª–æ–≤–µ–∫: {text}\nAI:",
                f"–î–∏–∞–ª–æ–≥: {text}\n–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ:"
            ])
        
        return prompts

    def prepare_smart_data(self, training_data: List[Dict]) -> List[str]:
        """–£–º–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ"""
        logger.info("üéØ –£–º–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        quality_texts = []
        processed_count = 0
        
        # –°–Ω–∞—á–∞–ª–∞ —Å–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –¥–ª–∏–Ω–µ —Å–æ–æ–±—â–µ–Ω–∏–π
        lengths = []
        for item in training_data:
            text = ""
            if isinstance(item, dict) and 'text' in item:
                text = item['text'].strip()
            elif isinstance(item, str):
                text = item.strip()
            
            if text and len(text) > 5:
                lengths.append(len(text))
        
        if lengths:
            avg_length = np.mean(lengths)
        else:
            avg_length = 50
        
        # –û—Ç–±–∏—Ä–∞–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        for item in training_data[:800]:
            text = ""
            if isinstance(item, dict) and 'text' in item:
                text = item['text'].strip()
            elif isinstance(item, str):
                text = item.strip()
            
            # –£–º–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
            if (text and len(text) > 8 and len(text) < 300 and 
                not re.search(r'http[s]?://', text) and
                text.count('?') <= 3 and 
                self.calculate_quality_score(text) > 0.4):
                
                prompts = self.create_smart_prompts(text)
                quality_texts.extend(prompts)
                processed_count += 1
        
        # –î—É–±–ª–∏—Ä—É–µ–º –ª—É—á—à–∏–µ –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        if len(quality_texts) < 200:
            multiplier = max(2, 200 // len(quality_texts))
            quality_texts = quality_texts * multiplier
        
        logger.info(f"üìä –û—Ç–æ–±—Ä–∞–Ω–æ {processed_count} –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π")
        logger.info(f"üìù –°–æ–∑–¥–∞–Ω–æ {len(quality_texts)} –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤")
        
        return quality_texts[:1000]

    def calculate_quality_score(self, text: str) -> float:
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–µ–∫—Å—Ç–∞"""
        score = 0.0
        
        # –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞
        if 15 <= len(text) <= 250:
            score += 0.3
        
        # –†—É—Å—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã
        russian_chars = len(re.findall(r'[–∞-—è–ê-–Ø—ë–Å]', text))
        if russian_chars / len(text) > 0.6:
            score += 0.3
        
        # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Å–∏–º–≤–æ–ª–æ–≤
        unique_chars = len(set(text))
        if unique_chars / len(text) > 0.5:
            score += 0.2
        
        # –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –º—É—Å–æ—Ä–∞
        if not re.search(r'[^\w\s–∞-—è–ê-–Ø—ë–Å.,!?;:\-\'"()]', text):
            score += 0.2
        
        return score

    def train_ultra_optimized(
        self, 
        training_data: List[Dict],
        model_save_path: str
    ):
        """–°–≤–µ—Ä—Ö–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤"""
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ —Å–≤–µ—Ä—Ö–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è...")
        start_time = time.time()
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —É–º–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        texts = self.prepare_smart_data(training_data)
        if len(texts) < 50:
            raise ValueError(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {len(texts)} –ø—Ä–∏–º–µ—Ä–æ–≤")
            
        # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
        dataset = UltraOptimizedDataset(texts, self.tokenizer)
        
        # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        training_args = {
            'num_train_epochs': 3,
            'early_stopping': True,
            'per_device_train_batch_size': 2,  # –£–º–µ–Ω—å—à–∏–ª–∏ –±–∞—Ç—á –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            'gradient_accumulation_steps': 8,  # –£–≤–µ–ª–∏—á–∏–ª–∏ accumulation
            'learning_rate': 5e-4,  # –£–º–µ–Ω—å—à–∏–ª–∏ LR –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            'warmup_ratio': 0.1,
            'weight_decay': 0.01,
            'max_grad_norm': 1.0,
        }
        
        # –°–æ–∑–¥–∞–µ–º DataLoader
        train_loader = DataLoader(
            dataset, 
            batch_size=training_args['per_device_train_batch_size'],
            shuffle=True,
            num_workers=0,
            pin_memory=True
        )
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
        optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=training_args['learning_rate'],
            weight_decay=training_args['weight_decay']
        )
        
        # –ü—Ä–æ—Å—Ç–æ–π –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫
        total_steps = len(train_loader) * training_args['num_train_epochs']
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, 
            T_max=total_steps
        )
        
        # –û–±—É—á–µ–Ω–∏–µ
        train_losses = []
        best_loss = float('inf')
        
        logger.info(f"üìä –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ {len(texts)} –ø—Ä–∏–º–µ—Ä–∞—Ö")
        logger.info(f"‚öôÔ∏è  –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {training_args}")
        
        global_step = 0

        patience = 3  # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –µ—Å–ª–∏ 2 —à–∞–≥–∞ –ø–æ–¥—Ä—è–¥ –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏–π
        best_loss = float('inf')
        patience_counter = 0

        for epoch in range(training_args['num_train_epochs']):
            epoch_start = time.time()
            self.model.train()  # –í–∞–∂–Ω–æ: –≤–∫–ª—é—á–∞–µ–º —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è
            
            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤–∫–ª—é—á–µ–Ω—ã
            for param in self.model.parameters():
                if param.requires_grad:
                    param.requires_grad = True
            
            total_loss = 0
            steps = 0
            
            for batch_idx, batch in enumerate(train_loader):
                # Move to device
                batch = {k: v.to(self.device) for k, v in batch.items()}
                
                # –û–±–Ω—É–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
                optimizer.zero_grad()
                
                # Forward pass
                outputs = self.model(
                    input_ids=batch['input_ids'],
                    attention_mask=batch['attention_mask'],
                    labels=batch['labels']
                )
                
                loss = outputs.loss
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ loss —Ç—Ä–µ–±—É–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
                if not loss.requires_grad:
                    logger.warning("Loss –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤! –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–æ–¥–µ–ª—å...")
                    # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –≤–∫–ª—é—á–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
                    loss.requires_grad = True
                
                # Gradient accumulation
                if training_args['gradient_accumulation_steps'] > 1:
                    loss = loss / training_args['gradient_accumulation_steps']
                
                # Backward pass
                loss.backward()
                
                # Step only on accumulation boundary
                if (batch_idx + 1) % training_args['gradient_accumulation_steps'] == 0:
                    # Gradient clipping
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(), 
                        training_args['max_grad_norm']
                    )
                    optimizer.step()
                    scheduler.step()
                    optimizer.zero_grad()
                    global_step += 1
                
                total_loss += loss.item()
                steps += 1
                
                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∫–∞–∂–¥—ã–µ 10 —à–∞–≥–æ–≤
                if batch_idx % 10 == 0:
                    current_loss = total_loss / (steps + 1e-8)
                    current_lr = scheduler.get_last_lr()[0]
                    logger.info(f'Epoch {epoch+1}, Step {batch_idx}, Loss: {current_loss:.4f}, LR: {current_lr:.2e}')
            
            # Final gradient update if needed
            if steps % training_args['gradient_accumulation_steps'] != 0:
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), training_args['max_grad_norm'])
                optimizer.step()
                scheduler.step()
                optimizer.zero_grad()
            
            # –≠–ø–æ—Ö–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞
            avg_loss = total_loss / steps
            train_losses.append(avg_loss)
            
            epoch_time = time.time() - epoch_start
            logger.info(f"‚úÖ Epoch {epoch+1}/{training_args['num_train_epochs']} - Loss: {avg_loss:.4f} - Time: {epoch_time:.1f}s")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
            if avg_loss < best_loss:
                best_loss = avg_loss
                self.save_model(model_save_path + "_best")
                logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å (loss: {best_loss:.4f})")

            #—Ä–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
            if avg_loss < best_loss:
                best_loss = avg_loss
                patience_counter = 0
                self.save_model(model_save_path + "_best")
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print("üõë –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ - –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –æ–ø—Ç–∏–º—É–º")
                    break
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        if os.path.exists(model_save_path + "_best"):
            self.model = None  # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å
            self.load_model(model_save_path + "_best")
        
        self.save_model(model_save_path)
        
        total_time = time.time() - start_time
        logger.info(f"üéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {total_time/60:.1f} –º–∏–Ω—É—Ç!")
        logger.info(f"üìà Final loss: {best_loss:.4f}")
        
        return {
            'train_losses': train_losses,
            'final_epoch': len(train_losses),
            'best_loss': best_loss,
            'total_time': total_time,
            'training_examples': len(texts)
        }

    def save_model(self, save_path: str):
        """–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        os.makedirs(save_path, exist_ok=True)
        self.model.save_pretrained(save_path)
        self.tokenizer.save_pretrained(save_path)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–µ
        training_info = {
            'saved_at': datetime.now().isoformat(),
            'model_type': 'rugpt3_lora_ultra_optimized',
            'trainable_parameters': sum(p.numel() for p in self.model.parameters() if p.requires_grad),
            'total_parameters': sum(p.numel() for p in self.model.parameters())
        }
        
        with open(os.path.join(save_path, 'training_info.json'), 'w') as f:
            json.dump(training_info, f, indent=2)
        
        logger.info(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {save_path}")

    def generate_response(self, message: str) -> str:
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤"""
        try:
            self.model.eval()  # –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º –≤ —Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏
            
            prompt = f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {message}\n–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:"
            
            inputs = self.tokenizer.encode(prompt, return_tensors="pt", max_length=128, truncation=True)
            inputs = inputs.to(self.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_length=len(inputs[0]) + 60,
                    num_return_sequences=1,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.2,
                    top_p=0.85,
                    top_k=40,
                    early_stopping=True
                )
            
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç–≤–µ—Ç
            if "–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:" in response:
                response = response.split("–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:")[-1].strip()
            elif "Assistant:" in response:
                response = response.split("Assistant:")[-1].strip()
            else:
                response = response.replace(prompt, "").strip()
            
            # –û—á–∏—Å—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞
            response = re.sub(r'^\s*[:\-]\s*', '', response)
            
            return response if response and len(response) > 2 else "–†–∞—Å—Å–∫–∞–∂–∏—Ç–µ –ø–æ–¥—Ä–æ–±–Ω–µ–µ!"
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            return "–î–∞–≤–∞–π—Ç–µ –ø–æ–≥–æ–≤–æ—Ä–∏–º –æ —á–µ–º-–Ω–∏–±—É–¥—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–º!"

# –ü—Ä–æ—Å—Ç–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
class OptimizedTrainer(UltraOptimizedTrainer):
    """–°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å–æ —Å—Ç–∞—Ä—ã–º –∫–æ–¥–æ–º"""
    def train_optimized(self, training_data, model_save_path):
        return self.train_ultra_optimized(training_data, model_save_path)