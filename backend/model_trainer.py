# model_trainer.py
import os
import json
import torch
import logging
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling
from torch.utils.data import Dataset
import gc

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SimpleDataset(Dataset):
    def __init__(self, tokenizer, data, max_length=256):
        self.tokenizer = tokenizer
        self.data = data
        self.max_length = max_length
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø—Ä–∏–º–µ—Ä –≤ —Ñ–æ—Ä–º–∞—Ç–µ –¥–∏–∞–ª–æ–≥–∞
        if item.get('input'):
            text = f"–í–æ–ø—Ä–æ—Å: {item['input']}\n–û—Ç–≤–µ—Ç: {item['output']}"
        else:
            text = f"–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è: {item['instruction']}\n–û—Ç–≤–µ—Ç: {item['output']}"
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        encoding = self.tokenizer(
            text,
            truncation=True,
            max_length=self.max_length,
            padding='max_length',
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': encoding['input_ids'].flatten()
        }

class RussianModelTrainer:
    """–¢—Ä–µ–Ω–µ—Ä –¥–ª—è —Ä—É—Å—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º–∏"""
    
    def __init__(self):
        self.model = None
        self.tokenizer = None
        
        # –¢–û–õ–¨–ö–û –†–£–°–°–ö–ò–ï –ò –ü–†–û–í–ï–†–ï–ù–ù–´–ï –ú–û–î–ï–õ–ò
        self.available_models = [
            "sberbank-ai/rugpt3small_based_on_gpt2",  # –†—É—Å—Å–∫–∞—è, —Å—Ç–∞–±–∏–ª—å–Ω–∞—è
            "tinkoff-ai/ruDialoGPT-small",           # –†—É—Å—Å–∫–∞—è –¥–∏–∞–ª–æ–≥–æ–≤–∞—è
            "ai-forever/rugpt3small_based_on_gpt2",  # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è —Ä—É—Å—Å–∫–∞—è
        ]
    
    def load_model(self, model_index: int = 0):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –Ω–∞ —Ä—É—Å—Å–∫–∏–µ –º–æ–¥–µ–ª–∏"""
        try:
            if model_index >= len(self.available_models):
                logger.error("‚ùå –í—Å–µ —Ä—É—Å—Å–∫–∏–µ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")
                return False
                
            model_name = self.available_models[model_index]
            logger.info(f"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å: {model_name}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            self.model = AutoModelForCausalLM.from_pretrained(model_name)
            
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}: {e}")
            
            # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
            
            # –ü—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å
            if model_index + 1 < len(self.available_models):
                logger.info("üîÑ –ü—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å...")
                return self.load_model(model_index + 1)
            else:
                logger.error("‚ùå –í—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")
                return False
    
    def train_model(self, user_id: str, training_data: list) -> dict:
        """–û—Å–Ω–æ–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        try:
            import time
            start_time = time.time()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞–Ω–Ω—ã–µ
            if len(training_data) < 10:
                return {
                    "success": False,
                    "message": f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö: {len(training_data)} –ø—Ä–∏–º–µ—Ä–æ–≤ (–Ω—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 10)",
                    "training_time": 0,
                    "samples_used": 0
                }
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            if not self.load_model():
                return {
                    "success": False,
                    "message": "–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å",
                    "training_time": 0,
                    "samples_used": 0
                }
            
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {self.tokenizer.name_or_path}")
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            limited_data = training_data[:500]  # –ú–∞–∫—Å–∏–º—É–º 500 –ø—Ä–∏–º–µ—Ä–æ–≤
            logger.info(f"üìä –ò—Å–ø–æ–ª—å–∑—É–µ–º {len(limited_data)} –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            
            # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
            dataset = SimpleDataset(self.tokenizer, limited_data)
            
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
            training_args = TrainingArguments(
                output_dir=f"trained_models/user_{user_id}",
                overwrite_output_dir=True,
                num_train_epochs=2,
                per_device_train_batch_size=2,
                gradient_accumulation_steps=2,
                warmup_steps=50,
                logging_steps=20,
                save_steps=100,
                learning_rate=3e-5,
                weight_decay=0.01,
                fp16=torch.cuda.is_available(),
                remove_unused_columns=False,
                report_to=None,
                save_total_limit=1,
                dataloader_drop_last=True,
            )
            
            # Data collator
            data_collator = DataCollatorForLanguageModeling(
                tokenizer=self.tokenizer,
                mlm=False,
            )
            
            # –¢—Ä–µ–Ω–µ—Ä
            trainer = Trainer(
                model=self.model,
                args=training_args,
                data_collator=data_collator,
                train_dataset=dataset,
                tokenizer=self.tokenizer,
            )
            
            # –û–±—É—á–∞–µ–º
            logger.info("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
            trainer.train()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º
            trainer.save_model()
            self.tokenizer.save_pretrained(f"trained_models/user_{user_id}")
            
            training_time = time.time() - start_time
            
            return {
                "success": True,
                "message": f"–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –Ω–∞ {len(limited_data)} –ø—Ä–∏–º–µ—Ä–∞—Ö",
                "model_path": f"trained_models/user_{user_id}",
                "training_time": round(training_time, 2),
                "samples_used": len(limited_data),
                "base_model": self.tokenizer.name_or_path,
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")
            return {
                "success": False,
                "message": f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {str(e)}",
                "training_time": 0,
                "samples_used": 0
            }

def train_user_model(user_id: str) -> dict:
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è –∏–∑–≤–Ω–µ"""
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        data_path = f"user_data/{user_id}/training_data_alpaca.json"
        if not os.path.exists(data_path):
            return {
                "success": False,
                "message": "–î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
            }
        
        with open(data_path, 'r', encoding='utf-8') as f:
            training_data = json.load(f)
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
        trainer = RussianModelTrainer()
        result = trainer.train_model(user_id, training_data)
        
        return result
        
    except Exception as e:
        return {
            "success": False,
            "message": f"–û—à–∏–±–∫–∞: {str(e)}"
        }

if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    user_id = "6209265331"
    result = train_user_model(user_id)
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")