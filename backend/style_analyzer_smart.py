# style_analyzer_smart_v2.py - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø
# ‚úÖ –£–±—Ä–∞–Ω–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è '–ø—Ä–∏–≤–µ—Ç', –¥–æ–±–∞–≤–ª–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫, –¥–æ–ø–æ–ª–Ω–µ–Ω—ã –ø—Ä–∏–º–µ—Ä—ã

import json
import chromadb
import re
from collections import Counter
from chromadb.utils import embedding_functions
from config import CHROMA_DB_DIR

def is_valid_response(msg):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –≤–∞–ª–∏–¥–Ω—ã–π –ª–∏ —ç—Ç–æ –æ—Ç–≤–µ—Ç"""
    if not msg or len(msg) < 2:
        return False
    
    # –°–ø–∞–º –º–∞—Ä–∫–µ—Ä—ã
    spam_markers = ['http', '–∫—É', '–∂–∂', 'uvk', 'jn', '***']
    if any(marker in msg.lower() for marker in spam_markers):
        return False
    
    # –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
    if '!!!!' in msg or '????' in msg:
        return False
    
    return True

def extract_style_features(messages):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç–∏–ª—å —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
    valid_messages = [msg for msg in messages if is_valid_response(msg)]
    
    if not valid_messages:
        return {
            "sentence_starts": [],
            "frequent_phrases": [],
            "punctuation": {},
            "avg_sentence_length": 50,
            "sentence_lengths": []
        }
    
    print(f"üìä –í–∞–ª–∏–¥–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π: {len(valid_messages)} –∏–∑ {len(messages)}")
    
    sentence_starts = []
    phrase_patterns = []
    punctuation_marks = Counter()
    sentence_lengths = []
    russian_stopwords = {
        '–∏', '–≤', '–Ω–∞', '–ø–æ', '–Ω–æ', '–∏–ª–∏', '—á—Ç–æ', '–∫–∞–∫', '–≥–¥–µ', '–∫–æ–≥–¥–∞',
        '–µ—Å–ª–∏', '—Ç–æ', '—ç—Ç–æ', '–Ω–µ', '–¥–∞', '–Ω–µ—Ç', '–∞', '–∫', '—Å', '–∑–∞'
    }
    
    for msg in valid_messages:
        msg_clean = msg.strip()
        
        # –ù–∞—á–∞–ª–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        words = msg_clean.split()[:3]
        if words and len(' '.join(words)) > 3:
            start = ' '.join(words).lower()
            sentence_starts.append(start)
        
        # –î–ª–∏–Ω–∞
        sentence_lengths.append(len(msg_clean))
        
        # –ü—É–Ω–∫—Ç—É–∞—Ü–∏—è
        for char in msg_clean:
            if char in '!?.,-;:()':
                punctuation_marks[char] += 1
        
        # –ß–∞—Å—Ç—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        words_lower = [w.lower() for w in words if w.lower() not in russian_stopwords and len(w) > 2]
        if len(words_lower) >= 2:
            phrase_patterns.append(' '.join(words_lower[:2]))
    
    top_starts = Counter(sentence_starts).most_common(15)
    filtered_starts = [(s, count) for s, count in top_starts if count >= 2]
    
    return {
        "sentence_starts": filtered_starts if filtered_starts else top_starts[:5],
        "frequent_phrases": Counter(phrase_patterns).most_common(10),
        "punctuation": dict(punctuation_marks.most_common(10)),
        "avg_sentence_length": sum(sentence_lengths) / len(sentence_lengths) if sentence_lengths else 50,
        "sentence_lengths": sentence_lengths
    }

def extract_tone_markers(messages):
    """–ò—â–µ—Ç –º–∞—Ä–∫–µ—Ä—ã —Ç–æ–Ω–∞"""
    tone_patterns = {
        "–∏—Ä–æ–Ω–∏—á–Ω—ã–π": {
            "markers": ['—â–∞—Å', '–ª–æ–ª', '–Ω—É –¥–∞', '–∫–æ–Ω–µ—á–Ω–æ', '–∫–µ–∫', '—Ö–∞—Ö–∞—Ö'],
            "count": 0
        },
        "–ø—Ä—è–º–æ–π": {
            "markers": ['–ø—Ä–æ—Å—Ç–æ', '–≤ –ø—Ä–∏–Ω—Ü–∏–ø–µ', '–≤ —Ü–µ–ª–æ–º', '–ø–æ —Ñ–∞–∫—Ç—É'],
            "count": 0
        },
        "–≤–æ–ø—Ä–æ—à–∞—é—â–∏–π": {
            "markers": ['–∞ –ø–æ—á–µ–º—É', '–∑–∞—á–µ–º', '–æ—Ç–∫—É–¥–∞', '–∫–∞–∫ —Ç–∞–∫'],
            "count": 0
        },
        "—Ç–≤–æ—Ä—á–µ—Å–∫–∏–π": {
            "markers": ['–º–æ–∂–µ—Ç', '–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ', '–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é', '–∑–≤—É—á–∏—Ç'],
            "count": 0
        },
        "—Å–µ—Ä—å–µ–∑–Ω—ã–π": {
            "markers": ['—Å—á–∏—Ç–∞—é', '–¥—É–º–∞—é', '–º–Ω–µ–Ω–∏–µ', '—É–±–µ–∂–¥–µ–Ω'],
            "count": 0
        }
    }
    
    msg_combined = ' '.join([m for m in messages if is_valid_response(m)]).lower()
    
    for tone, data in tone_patterns.items():
        for marker in data["markers"]:
            count = msg_combined.count(marker)
            tone_patterns[tone]["count"] += count
    
    sorted_tones = sorted(
        [(tone, data["count"]) for tone, data in tone_patterns.items()],
        key=lambda x: x[1],
        reverse=True
    )
    
    # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ–Ω—ã —Å —Ö–æ—Ç—è –±—ã 1 –º–∞—Ä–∫–µ—Ä–æ–º
    return [(tone, count) for tone, count in sorted_tones if count > 0]

def extract_response_patterns(messages):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã –æ—Ç–≤–µ—Ç–æ–≤"""
    valid_messages = [msg for msg in messages if is_valid_response(msg)]
    
    response_patterns = {
        "greeting": [],
        "status": [],
        "identity": [],
        "action": [],
        "opinion": []
    }
    
    for msg in valid_messages:
        msg_lower = msg.lower().strip()
        
        # –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è
        if any(x in msg_lower for x in ['–ø—Ä–∏–≤–µ—Ç', '—Ö–∞–π', 'hello', '—Å–∞–ª—é—Ç', '–ø–æ–∫–∞']):
            if 2 < len(msg) < 50:
                response_patterns["greeting"].append(msg.strip())
        
        # –°—Ç–∞—Ç—É—Å
        elif any(x in msg_lower for x in ['–Ω–æ—Ä–º', '–ø–æ–π–¥—ë—Ç', '—Ö–æ—Ä–æ—à–æ', '–æ—Ç–ª–∏—á–Ω–æ', '—Ç–∞–∫ —Å–µ–±–µ']):
            if 2 < len(msg) < 100:
                response_patterns["status"].append(msg.strip())
        
        # –õ–∏—á–Ω–æ—Å—Ç—å
        elif any(x in msg_lower for x in ['—è ', '–∏–º—è', '–∑–æ–≤—É—Ç', '–∑–æ–≤—É', '–∞–Ω–¥—Ä–µ–π']):
            if 2 < len(msg) < 100:
                response_patterns["identity"].append(msg.strip())
        
        # –î–µ–π—Å—Ç–≤–∏—è
        elif any(x in msg_lower for x in ['–¥–µ–ª–∞—é', '–∑–∞–Ω—è—Ç', '–∫–æ–¥–∏—Ä—É—é', '—Å–º–æ—Ç—Ä—é', '—Ä–∞–∑–≤–ª–µ–∫–∞—é—Å—å']):
            if 5 < len(msg) < 200:
                response_patterns["action"].append(msg.strip())
        
        # –ú–Ω–µ–Ω–∏—è
        elif any(x in msg_lower for x in ['–¥—É–º–∞—é', '—Å—á–∏—Ç–∞—é', '–º–Ω–µ–Ω–∏–µ', '–∫–∞–∂–µ—Ç—Å—è']):
            if 5 < len(msg) < 200:
                response_patterns["opinion"].append(msg.strip())
    
    # –û—á–∏—â–∞–µ–º –∏ –±–µ—Ä–µ–º –¢–û–ü
    for key in response_patterns:
        unique_patterns = list(set(response_patterns[key]))
        response_patterns[key] = unique_patterns[:5]
    
    return response_patterns

def build_smart_system_prompt(facts, style_features, tone_markers, response_patterns):
    """–°–æ–∑–¥–∞–µ—Ç —É–º–Ω—ã–π –ø—Ä–æ–º—Ç"""
    personal = facts.get('personal', {})
    location = facts.get('location', {})
    hobbies = facts.get('hobbies', {})
    beliefs = facts.get('beliefs', {})
    social = facts.get('social', {})
    
    # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    common_starts = style_features.get('sentence_starts', [])
    starts_str = ', '.join([f'"{start}"' for start, _ in common_starts[:4]]) if common_starts else '"–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π"'
    
    punctuation = style_features.get('punctuation', {})
    main_punct = max(punctuation.items(), key=lambda x: x[1])[0] if punctuation else '.'
    
    tones = [tone for tone, _ in tone_markers[:2]]
    tone_desc = ', '.join(tones) if tones else '–ø—Ä—è–º–æ–π'
    
    avg_len = style_features.get('avg_sentence_length', 60)
    
    # –ü—Ä–∏–º–µ—Ä—ã –æ—Ç–≤–µ—Ç–æ–≤ —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –ø—É—Å—Ç–æ—Ç—ã
    greeting_examples = response_patterns.get('greeting', [f'–ü—Ä–∏–≤–µ—Ç{main_punct}'])
    status_examples = response_patterns.get('status', [f'–ù–æ—Ä–º{main_punct}'])
    action_examples = response_patterns.get('action', ['–ö–æ–¥–∏—Ä—É—é –æ–±—ã—á–Ω–æ.'])
    
    if not greeting_examples:
        greeting_examples = [f'–ü—Ä–∏–≤–µ—Ç{main_punct}']
    if not status_examples:
        status_examples = [f'–ù–æ—Ä–º{main_punct}']
    
    system_prompt = f"""–¢—ã ‚Äî {personal.get('full_name', '–ê–Ω–¥—Ä–µ–π')}, {personal.get('age', '25')} –ª–µ—Ç, –∏–∑ {location.get('current_city', '–ú–æ—Å–∫–≤—ã')}.

=== –¢–í–û–ô –°–¢–ò–õ–¨ (–†–ï–ê–õ–¨–ù–´–ô) ===

–ú–ê–ù–ï–†–ê –†–ï–ß–ò:
- –¢–æ–Ω: {tone_desc}
- –ö–∞–∫ –Ω–∞—á–∏–Ω–∞–µ—à—å: {starts_str}
- –ü—É–Ω–∫—Ç—É–∞—Ü–∏—è: {main_punct}
- –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: ~{round(avg_len)} —Å–∏–º–≤–æ–ª–æ–≤
- –°—Ç–∏–ª—å: –ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π, –±–µ–∑ —à–∞–±–ª–æ–Ω–æ–≤

–§–ê–ö–¢–´ –û –°–ï–ë–ï:
- –õ—é–±–ª—é: {', '.join(hobbies.get('likes', ['–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ'])[:5])}
- –ú—É–∑—ã–∫–∞: {hobbies.get('music', '—Ä–∞–∑–Ω–∞—è')}
- –ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ: {'–¥–∞' if hobbies.get('programming') else '–Ω–µ—Ç'}
- –°—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–µ –∏–≥—Ä—ã: {'–¥–∞' if hobbies.get('strategy_games') else '–Ω–µ—Ç'}
- –î—Ä—É–∑—å—è: {', '.join(social.get('friends', [])[:3]) if social.get('friends') else '–µ—Å—Ç—å'}
- –õ—É—á—à–∏–π –¥—Ä—É–≥: {social.get('best_friend', '–Ω–µ —É–∫–∞–∑–∞–Ω')}

–£–ë–ï–ñ–î–ï–ù–ò–Ø:
{'; '.join(beliefs.get('core_beliefs', [])[:2]) if beliefs.get('core_beliefs') else '–¶–µ–Ω—é —á–µ—Å—Ç–Ω–æ—Å—Ç—å –∏ –ª–æ–≥–∏–∫—É'}

=== –ü–†–ò–ú–ï–†–´ –¢–í–û–ò–• –û–¢–í–ï–¢–û–í ===

–ù–∞ "–ø—Ä–∏–≤–µ—Ç":
{chr(10).join(['  ' + ex for ex in greeting_examples[:2]])}

–ù–∞ "–∫–∞–∫ –¥–µ–ª–∞":
{chr(10).join(['  ' + ex for ex in status_examples[:2]])}

–ù–∞ "—á—Ç–æ –¥–µ–ª–∞–µ—à—å":
{chr(10).join(['  ' + ex for ex in action_examples[:2]])}

=== –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ü–†–ê–í–ò–õ–ê ===

1. –û—Ç–≤–µ—á–∞–π –ö–û–†–û–¢–ö–û - –æ–¥–Ω–∞-–¥–≤–µ —Ñ—Ä–∞–∑—ã –º–∞–∫—Å–∏–º—É–º
2. –ë—É–¥—å {tone_desc}
3. –ò—Å–ø–æ–ª—å–∑—É–π —Ñ–∞–∫—Ç—ã –æ —Å–µ–±–µ —Ç–æ–ª—å–∫–æ –∫–æ–≥–¥–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ
4. –ù–ï –≤—ã—Ö–æ–¥–∏ –∑–∞ —Ä–∞–º–∫–∏ —Å–≤–æ–µ–≥–æ —Å—Ç–∏–ª—è
5. –ü–æ–º–Ω–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
6. –ó–ê–ü–†–ï–©–ï–ù–û: —Å—Ç—Ä–∞–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã, –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è, –Ω–µ—á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–µ –æ—Ç–≤–µ—Ç—ã
7. –ó–∞–∫–∞–Ω—á–∏–≤–∞–π –ø—É–Ω–∫—Ç—É–∞—Ü–∏–µ–π: {main_punct}
"""
    
    return system_prompt

def analyze_and_build_smart():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞"""
    try:
        with open('data/facts_advanced.json', 'r', encoding='utf-8') as f:
            facts = json.load(f)
    except FileNotFoundError:
        print("‚ùå facts_advanced.json –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        facts = {'personal': {}, 'location': {}, 'hobbies': {}, 'social': {}, 'beliefs': {}}
    
    # ChromaDB
    try:
        client = chromadb.PersistentClient(path=CHROMA_DB_DIR)
        embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name="paraphrase-multilingual-MiniLM-L12-v2"
        )
        collection = client.get_collection(name="user_messages", embedding_function=embedding_function)
        all_docs = collection.get(include=['documents', 'embeddings'])
        messages = all_docs.get('documents', [])
    except Exception as e:
        print(f"‚ö†Ô∏è ChromaDB –æ—à–∏–±–∫–∞: {e}")
        messages = []
    
    print(f"üìä –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é {len(messages)} —Å–æ–æ–±—â–µ–Ω–∏–π...")
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
    style_features = extract_style_features(messages)
    print(f"‚úÖ –°—Ç–∏–ª—å: {len(style_features['sentence_starts'])} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    tone_markers = extract_tone_markers(messages)
    print(f"‚úÖ –¢–æ–Ω: {[t for t, _ in tone_markers[:2]]}")
    
    response_patterns = extract_response_patterns(messages)
    print(f"‚úÖ –ü–∞—Ç—Ç–µ—Ä–Ω—ã –Ω–∞–π–¥–µ–Ω—ã")
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º—Ç
    system_prompt = build_smart_system_prompt(facts, style_features, tone_markers, response_patterns)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    template = {
        "system_prompt": system_prompt,
        "style_analysis": {
            "sentence_starts": [s for s, _ in style_features['sentence_starts']],
            "tone": [t for t, _ in tone_markers],
            "avg_length": style_features['avg_sentence_length'],
            "response_patterns": response_patterns
        },
        "created_at": __import__('datetime').datetime.now().isoformat()
    }
    
    with open('data/prompt_template.json', 'w', encoding='utf-8') as f:
        json.dump(template, f, ensure_ascii=False, indent=2)
    
    print("\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
    return template

def load_prompt_template():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —à–∞–±–ª–æ–Ω"""
    try:
        with open('data/prompt_template.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print("‚ö†Ô∏è prompt_template.json –Ω–µ –Ω–∞–π–¥–µ–Ω! –ó–∞–ø—É—Å—Ç–∏ analyze_and_build_smart()")
        return None

if __name__ == "__main__":
    template = analyze_and_build_smart()
