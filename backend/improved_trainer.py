# improved_trainer.py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from torch.utils.data import Dataset, DataLoader
import json
import os
import logging
import re

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CleanDataset(Dataset):
    def __init__(self, texts, tokenizer, max_length=128):
        self.tokenizer = tokenizer
        self.texts = texts
        self.max_length = max_length
        
    def __len__(self):
        return len(self.texts)
        
    def __getitem__(self, idx):
        text = self.texts[idx]
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': encoding['input_ids'].flatten()
        }

class ImprovedTrainer:
    def __init__(self, model_name: str = "sberbank-ai/rugpt3small_based_on_gpt2"):
        self.model_name = model_name
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        self.model = None
        self.tokenizer = None
        
    def load_model(self, model_path: str = None):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        try:
            logger.info("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
            
            if model_path and os.path.exists(model_path):
                self.model = AutoModelForCausalLM.from_pretrained(model_path)
                self.tokenizer = AutoTokenizer.from_pretrained(model_path)
            else:
                self.model = AutoModelForCausalLM.from_pretrained(self.model_name)
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
                
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                
            self.model.to(self.device)
            logger.info("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            raise

    def clean_text(self, text):
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç —Å—Å—ã–ª–æ–∫ –∏ –º—É—Å–æ—Ä–∞"""
        # –£–¥–∞–ª—è–µ–º URL
        text = re.sub(r'http[s]?://\S+', '', text)
        text = re.sub(r'www\.\S+', '', text)
        text = re.sub(r'\S+\.(com|ru|org|net)\S*', '', text)
        
        # –£–¥–∞–ª—è–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∏ —Ö–µ—à—Ç–µ–≥–∏
        text = re.sub(r'@\w+', '', text)
        text = re.sub(r'#\w+', '', text)
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text

    def prepare_clean_data(self, training_data):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ—á–∏—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        texts = []
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –í–°–ï —Å–æ–æ–±—â–µ–Ω–∏—è (–±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è 200)
        logger.info(f"üéØ –ò—Å–ø–æ–ª—å–∑—É–µ–º {len(training_data)} —Å–æ–æ–±—â–µ–Ω–∏–π")
        
        # –°–æ–∑–¥–∞–µ–º —á–∏—Å—Ç—ã–µ –ø–∞—Ä—ã –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç
        for i in range(len(training_data) - 1):
            current_msg = self.extract_text(training_data[i])
            next_msg = self.extract_text(training_data[i + 1])
            
            # –û—á–∏—â–∞–µ–º –æ—Ç —Å—Å—ã–ª–æ–∫ –∏ –º—É—Å–æ—Ä–∞
            current_msg = self.clean_text(current_msg)
            next_msg = self.clean_text(next_msg)
            
            if (current_msg and next_msg and 
                len(current_msg) > 5 and len(next_msg) > 5 and
                self.is_russian_text(current_msg) and self.is_russian_text(next_msg)):
                
                dialog = f"–ß–µ–ª–æ–≤–µ–∫: {current_msg}\nAI: {next_msg}"
                texts.append(dialog)
        
        logger.info(f"üìä –°–æ–∑–¥–∞–Ω–æ {len(texts)} —á–∏—Å—Ç—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤")
        return texts

    def extract_text(self, item):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞"""
        if isinstance(item, dict):
            return item.get('text', '').strip()
        elif isinstance(item, str):
            return item.strip()
        return ""

    def is_russian_text(self, text):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ —Ä—É—Å—Å–∫–∏–º"""
        russian_chars = len(re.findall(r'[–∞-—è–ê-–Ø—ë–Å]', text))
        total_chars = len(re.sub(r'\s', '', text))
        
        if total_chars == 0:
            return False
            
        return (russian_chars / total_chars) > 0.6

    def train_improved(
        self, 
        training_data,
        model_save_path: str,
        epochs: int = 3,
        batch_size: int = 2,
        learning_rate: float = 5e-5
    ):
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –æ—á–∏—â–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏"""
        logger.info("üîÑ –£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ...")
        
        texts = self.prepare_clean_data(training_data)
        
        if not texts or len(texts) < 10:
            logger.warning("–ú–∞–ª–æ —á–∏—Å—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–µ")
            texts = self.prepare_data(training_data)
        
        train_dataset = CleanDataset(texts, self.tokenizer)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate)
        
        train_losses = []
        
        for epoch in range(epochs):
            self.model.train()
            total_loss = 0
            batches = 0
            
            for batch in train_loader:
                optimizer.zero_grad()
                
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )
                
                loss = outputs.loss
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
                batches += 1
                
                if batches % 20 == 0:
                    logger.info(f'Epoch {epoch+1}, Batch {batches}, Loss: {loss.item():.4f}')
            
            if batches > 0:
                avg_loss = total_loss / batches
                train_losses.append(avg_loss)
                logger.info(f"‚úÖ Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}")
        
        self.save_model(model_save_path)
        
        return {
            'train_losses': train_losses,
            'final_epoch': epochs,
            'final_loss': train_losses[-1] if train_losses else 0.0
        }

    def prepare_data(self, training_data):
        """–†–µ–∑–µ—Ä–≤–Ω—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
        texts = []
        for i in range(len(training_data) - 1):
            current_msg = self.extract_text(training_data[i])
            next_msg = self.extract_text(training_data[i + 1])
            
            if current_msg and next_msg and len(current_msg) > 3 and len(next_msg) > 3:
                dialog = f"–ß–µ–ª–æ–≤–µ–∫: {current_msg}\nAI: {next_msg}"
                texts.append(dialog)
        
        return texts

    def save_model(self, save_path: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        os.makedirs(save_path, exist_ok=True)
        self.model.save_pretrained(save_path)
        self.tokenizer.save_pretrained(save_path)
        logger.info(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {save_path}")

def improved_retrain():
    """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ"""
    user_id = "6209265331"
    
    print("üîÑ –£–ª—É—á—à–µ–Ω–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ...")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    data_file = f"user_data/{user_id}/training_data.json"
    with open(data_file, 'r', encoding='utf-8') as f:
        messages = json.load(f)
    
    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—É—é –º–æ–¥–µ–ª—å
    model_path = f"trained_models/user_{user_id}"
    if os.path.exists(model_path):
        import shutil
        shutil.rmtree(model_path)
        print("üóëÔ∏è –£–¥–∞–ª–µ–Ω–∞ —Å—Ç–∞—Ä–∞—è –º–æ–¥–µ–ª—å")
    
    # –û–±—É—á–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–º –º–µ—Ç–æ–¥–æ–º
    trainer = ImprovedTrainer(model_name="sberbank-ai/rugpt3small_based_on_gpt2")
    trainer.load_model()
    results = trainer.train_improved(messages, model_path, epochs=4)
    
    print(f"‚úÖ –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"üìä Loss: {results['final_loss']:.4f}")
    print(f"üéØ –≠–ø–æ—Ö: {results['final_epoch']}")
    
    return results

if __name__ == "__main__":
    improved_retrain()