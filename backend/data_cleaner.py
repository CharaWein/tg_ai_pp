# data_cleaner.py
import json
import re
import os
from typing import List, Dict

def ultra_clean_training_data(user_id: str = "6209265331"):
    """–°–≤–µ—Ä—Ö—Ç—â–∞—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    data_file = f"user_data/{user_id}/training_data.json"
    
    with open(data_file, 'r', encoding='utf-8') as f:
        messages = json.load(f)
    
    print(f"üìù –î–æ –æ—á–∏—Å—Ç–∫–∏: {len(messages)} —Å–æ–æ–±—â–µ–Ω–∏–π")
    
    cleaned_messages = []
    removed_count = 0
    
    for msg in messages:
        if isinstance(msg, dict):
            text = msg.get('text', '').strip()
            
            # –ñ–µ—Å—Ç–∫–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
            if not is_high_quality_text(text):
                removed_count += 1
                continue
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
            text = normalize_text(text)
            msg['text'] = text
            cleaned_messages.append(msg)
    
    print(f"‚úÖ –ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {len(cleaned_messages)} —Å–æ–æ–±—â–µ–Ω–∏–π")
    print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ: {removed_count} —Å–æ–æ–±—â–µ–Ω–∏–π")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    with open(data_file, 'w', encoding='utf-8') as f:
        json.dump(cleaned_messages, f, ensure_ascii=False, indent=2)
    
    return cleaned_messages

def is_high_quality_text(text: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ"""
    if not text or len(text) < 10 or len(text) > 500:
        return False
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
    russian_ratio = check_russian_ratio(text)
    if russian_ratio < 0.6:
        return False
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –º—É—Å–æ—Ä
    if contains_gibberish(text):
        return False
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
    if has_repetitions(text):
        return False
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
    if re.search(r'[{}[\]<>|\\]', text):
        return False
    
    return True

def check_russian_ratio(text: str) -> float:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø—Ä–æ—Ü–µ–Ω—Ç —Ä—É—Å—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
    russian_chars = len(re.findall(r'[–∞-—è–ê-–Ø—ë–Å]', text))
    total_chars = len(re.sub(r'\s', '', text))
    
    if total_chars == 0:
        return 0
    
    return russian_chars / total_chars

def normalize_text(text: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r'\s+', ' ', text).strip()
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫–∞–≤—ã—á–∫–∏
    text = re.sub(r'[¬´¬ª""]', '"', text)
    
    # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
    text = re.sub(r'([!?])\1+', r'\1', text)
    
    return text

def contains_gibberish(text: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç"""
    # –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Å–∏–º–≤–æ–ª–æ–≤
    if re.search(r'(.)\1{4,}', text):
        return True
    
    # –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
    special_chars = len(re.findall(r'[^\w\s–∞-—è–ê-–Ø—ë–Å.,!?;:\-\'"()]', text))
    if special_chars / len(text) > 0.1:
        return True
    
    return False

def has_repetitions(text: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–ª–æ–≤–∞"""
    words = text.lower().split()
    if len(words) < 3:
        return False
    
    word_counts = {}
    for word in words:
        if len(word) > 3:
            word_counts[word] = word_counts.get(word, 0) + 1
            if word_counts[word] > 3:
                return True
    
    return False

# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
def clean_non_russian_data(user_id: str = "6209265331"):
    """–£–¥–∞–ª—è–µ–º –≤—Å–µ –Ω–µ-—Ä—É—Å—Å–∫–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è (—Å—Ç–∞—Ä–∞—è –≤–µ—Ä—Å–∏—è)"""
    return ultra_clean_training_data(user_id)

if __name__ == "__main__":
    ultra_clean_training_data()