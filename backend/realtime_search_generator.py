# realtime_search_generator.py
import torch
import re
import os
import json
import numpy as np
from typing import List, Tuple
from transformers import AutoTokenizer, AutoModelForCausalLM

class RealtimeSearchGenerator:
    def __init__(self, model, tokenizer, user_id: str):
        self.model = model
        self.tokenizer = tokenizer
        self.user_id = user_id
        self.knowledge = self.load_knowledge()

    def load_knowledge(self) -> dict:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –±–∞–∑—É –∑–Ω–∞–Ω–∏–π —Å —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏"""
        knowledge_path = f"user_data/{self.user_id}/knowledge_base.json"
        if os.path.exists(knowledge_path):
            try:
                with open(knowledge_path, 'r', encoding='utf-8') as f:
                    knowledge = json.load(f)
                    print(f"üìö –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(knowledge.get('all_messages', []))} —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –ø–æ–∏—Å–∫–∞")
                    return knowledge
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∑–Ω–∞–Ω–∏–π: {e}")
        return {"personal_info": {}, "interests": [], "all_messages": []}

    def generate_response(self, message: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å real-time –ø–æ–∏—Å–∫–æ–º —Ñ–∞–∫—Ç–æ–≤"""
        # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Ç–æ—á–Ω—ã–π –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        keyword_answer = self.enhanced_keyword_search(message)
        if keyword_answer and self.is_good_answer(keyword_answer):
            print(f"üîç –£–º–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–∞—à–µ–ª –æ—Ç–≤–µ—Ç: {keyword_answer}")
            return keyword_answer
        
        # –ï—Å–ª–∏ –ø–æ–∏—Å–∫ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ–±—ã—á–Ω—ã–º —Å–ø–æ—Å–æ–±–æ–º
        return self.fallback_generation(message)

    def enhanced_keyword_search(self, question: str) -> str:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –æ—Ç–≤–µ—Ç–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        question_lower = question.lower()
        all_messages = self.knowledge.get("all_messages", [])
        
        if not all_messages:
            return ""
        
        # –£–ª—É—á—à–µ–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –≤–æ–ø—Ä–æ—Å–æ–≤
        keyword_groups = {
            '–∏–º—è': {
                'keywords': ['–∑–æ–≤—É—Ç', '–∏–º—è', '–∫–∞–∫ —Ç–µ–±—è', '–ø—Ä–µ–¥—Å—Ç–∞–≤—å—Å—è', '—Ç–≤–æ–µ –∏–º—è'],
                'response_patterns': ['–º–µ–Ω—è –∑–æ–≤—É—Ç', '–º–æ–µ –∏–º—è', '—è -', '–∑–æ–≤–∏—Ç–µ –º–µ–Ω—è'],
                'extract_patterns': [
                    r'–º–µ–Ω—è –∑–æ–≤—É—Ç\s+([–ê-–Ø][–∞-—è]{2,})',
                    r'–º–æ–µ –∏–º—è\s+([–ê-–Ø][–∞-—è]{2,})',
                    r'—è\s+([–ê-–Ø][–∞-—è]{2,})',
                    r'–∑–æ–≤–∏—Ç–µ –º–µ–Ω—è\s+([–ê-–Ø][–∞-—è]{2,})'
                ]
            },
            '–≥–æ—Ä–æ–¥': {
                'keywords': ['–≥–æ—Ä–æ–¥', '–∂–∏–≤–µ—à—å', '–æ—Ç–∫—É–¥–∞', '–≥–¥–µ –∂–∏–≤–µ—à—å', '–º–µ—Å—Ç–æ–∂–∏—Ç–µ–ª—å—Å—Ç–≤–æ'],
                'response_patterns': ['–∂–∏–≤—É –≤', '–≥–æ—Ä–æ–¥', '–∏–∑ –≥–æ—Ä–æ–¥–∞', '–≤ –≥–æ—Ä–æ–¥–µ'],
                'extract_patterns': [
                    r'–∂–∏–≤—É –≤\s+([–ê-–Ø][–∞-—è]+)',
                    r'–≥–æ—Ä–æ–¥\s+([–ê-–Ø][–∞-—è]+)',
                    r'–∏–∑\s+([–ê-–Ø][–∞-—è]+)',
                    r'–≤\s+([–ê-–Ø][–∞-—è]+)\s+–∂–∏–≤—É'
                ]
            },
            '–≤–æ–∑—Ä–∞—Å—Ç': {
                'keywords': ['–ª–µ—Ç', '–≤–æ–∑—Ä–∞—Å—Ç', '—Å–∫–æ–ª—å–∫–æ –ª–µ—Ç', '–≤–æ–∑—Ä–∞—Å—Ç —Ç–µ–±–µ'],
                'response_patterns': ['–º–Ω–µ –ª–µ—Ç', '–≤–æ–∑—Ä–∞—Å—Ç', '–º–Ω–µ –≥–æ–¥', '–ª–µ—Ç'],
                'extract_patterns': [
                    r'–º–Ω–µ\s+(\d{1,2})\s+–ª–µ—Ç',
                    r'–≤–æ–∑—Ä–∞—Å—Ç\s+(\d{1,2})',
                    r'(\d{1,2})\s+–≥–æ–¥'
                ]
            },
            '–∏–Ω—Ç–µ—Ä–µ—Å—ã': {
                'keywords': ['–∏–Ω—Ç–µ—Ä–µ—Å', '—É–≤–ª–µ–∫–∞–µ—à—å—Å—è', '—Ö–æ–±–±–∏', '–Ω—Ä–∞–≤–∏—Ç—Å—è', '–ª—é–±–∏—à—å'],
                'response_patterns': ['–Ω—Ä–∞–≤–∏—Ç—Å—è', '–ª—é–±–ª—é', '—É–≤–ª–µ–∫–∞—é—Å—å', '–∏–Ω—Ç–µ—Ä–µ—Å—É—é—Å—å'],
                'extract_patterns': [
                    r'–Ω—Ä–∞–≤–∏—Ç—Å—è\s+([^.,!?]+)',
                    r'–ª—é–±–ª—é\s+([^.,!?]+)',
                    r'—É–≤–ª–µ–∫–∞—é—Å—å\s+([^.,!?]+)',
                    r'–∏–Ω—Ç–µ—Ä–µ—Å—É—é—Å—å\s+([^.,!?]+)'
                ]
            },
            '—Ä–∞–±–æ—Ç–∞': {
                'keywords': ['—Ä–∞–±–æ—Ç–∞', '–ø—Ä–æ—Ñ–µ—Å—Å–∏—è', '–∑–∞–Ω–∏–º–∞–µ—à—å—Å—è', '—É—á—É—Å—å', '–¥–µ–ª–æ–º'],
                'response_patterns': ['—Ä–∞–±–æ—Ç–∞—é', '—É—á—É—Å—å', '–∑–∞–Ω–∏–º–∞—é—Å—å', '–ø—Ä–æ—Ñ–µ—Å—Å–∏—è'],
                'extract_patterns': [
                    r'—Ä–∞–±–æ—Ç–∞—é\s+([^.,!?]+)',
                    r'—É—á—É—Å—å\s+([^.,!?]+)',
                    r'–∑–∞–Ω–∏–º–∞—é—Å—å\s+([^.,!?]+)'
                ]
            },
            '–¥—Ä—É–∑—å—è': {
                'keywords': ['–¥—Ä—É–≥', '–¥—Ä—É–∑—å—è', '–æ–±—â–∞–µ—à—å—Å—è', '–∑–Ω–∞–∫–æ–º', '–ø–æ–¥—Ä—É–≥'],
                'response_patterns': ['–¥—Ä—É–≥', '–¥—Ä—É–∑—å—è', '–∑–Ω–∞–∫–æ–º', '–æ–±—â–∞—é—Å—å'],
                'extract_patterns': [
                    r'–¥—Ä—É–≥\s+([–ê-–Ø][–∞-—è]{2,})',
                    r'–¥—Ä—É–∑—å—è\s+([–ê-–Ø][–∞-—è]{2,})',
                    r'–∑–Ω–∞–∫–æ–º\s+([–ê-–Ø][–∞-—è]{2,})'
                ]
            }
        }
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –≤–æ–ø—Ä–æ—Å–∞
        question_category = None
        for category, data in keyword_groups.items():
            if any(keyword in question_lower for keyword in data['keywords']):
                question_category = category
                break
        
        if not question_category:
            return ""
        
        # –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        relevant_messages = []
        response_patterns = keyword_groups[question_category]['response_patterns']
        
        for msg in all_messages:
            msg_lower = msg.lower()
            # –ò—â–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –æ—Ç–≤–µ—Ç–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            if any(pattern in msg_lower for pattern in response_patterns):
                relevant_messages.append(msg)
        
        # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è, —Ñ–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        if relevant_messages:
            return self.construct_smart_answer(question, relevant_messages[:3], question_category, keyword_groups[question_category]['extract_patterns'])
        
        return ""
    
    def contains_gibberish(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –±–µ—Å—Å–º—ã—Å–ª–∏—Ü—É"""
        if len(text) < 5:
            return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–∏–º–≤–æ–ª—ã
        if re.search(r'(.)\1{3,}', text):
            return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Ä—É—Å—Å–∫–∏—Ö –±—É–∫–≤
        russian_chars = len(re.findall(r'[–∞-—è–ê-–Ø—ë–Å]', text))
        total_chars = max(len(re.findall(r'\w', text)), 1)
        
        if russian_chars / total_chars < 0.5:
            return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Å–º—ã—Å–ª–æ–≤—ã—Ö —Å–ª–æ–≤
        meaningful_words = ['–ø—Ä–∏–≤–µ—Ç', '–∫–∞–∫', '—á—Ç–æ', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–¥–∞', '–Ω–µ—Ç', '—Ö–æ—Ä–æ—à–æ', '—Å–ø–∞—Å–∏–±–æ']
        text_lower = text.lower()
        if not any(word in text_lower for word in meaningful_words):
            return len(text) > 30  # –î–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –±–µ–∑ —Å–º—ã—Å–ª–æ–≤—ã—Ö —Å–ª–æ–≤ = –±—Ä–µ–¥
        
        return False

    def construct_smart_answer(self, question: str, messages: List[str], category: str, extract_patterns: List[str]) -> str:
        """–£–º–Ω–æ–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π"""
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏–π
        extracted_info = self.extract_info_from_messages(messages, extract_patterns)
        
        if not extracted_info:
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤–æ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
            response = self.clean_context_response(messages[0])
        else:
            # –§–æ—Ä–º–∏—Ä—É–µ–º –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
            answer_templates = {
                '–∏–º—è': [
                    "–ú–µ–Ω—è –∑–æ–≤—É—Ç {info}",
                    "–Ø {info}",
                    "–ú–æ–µ –∏–º—è - {info}"
                ],
                '–≥–æ—Ä–æ–¥': [
                    "–Ø –∂–∏–≤—É –≤ {info}",
                    "–ú–æ–π –≥–æ—Ä–æ–¥ - {info}",
                    "–Ø –∏–∑ {info}"
                ],
                '–≤–æ–∑—Ä–∞—Å—Ç': [
                    "–ú–Ω–µ {info} –ª–µ—Ç",
                    "–ú–æ–π –≤–æ–∑—Ä–∞—Å—Ç - {info}",
                    "–ú–Ω–µ {info}"
                ],
                '–∏–Ω—Ç–µ—Ä–µ—Å—ã': [
                    "–Ø —É–≤–ª–µ–∫–∞—é—Å—å {info}",
                    "–ú–Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è {info}",
                    "–ú–æ–∏ –∏–Ω—Ç–µ—Ä–µ—Å—ã: {info}"
                ],
                '—Ä–∞–±–æ—Ç–∞': [
                    "–Ø –∑–∞–Ω–∏–º–∞—é—Å—å {info}",
                    "–ú–æ—è —Ä–∞–±–æ—Ç–∞ —Å–≤—è–∑–∞–Ω–∞ —Å {info}",
                    "–Ø —Ä–∞–±–æ—Ç–∞—é –≤ —Å—Ñ–µ—Ä–µ {info}"
                ],
                '–¥—Ä—É–∑—å—è': [
                    "–£ –º–µ–Ω—è –µ—Å—Ç—å –¥—Ä—É–∑—å—è: {info}",
                    "–Ø –æ–±—â–∞—é—Å—å —Å {info}",
                    "–ú–æ–∏ –¥—Ä—É–∑—å—è: {info}"
                ]
            }
            
            templates = answer_templates.get(category, ["{info}"])
            import random
            template = random.choice(templates)
            response = template.format(info=extracted_info)
        
        # –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –û–ë–†–ï–ó–ê–ï–ú –î–û –û–î–ù–û–ì–û –ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø
        return self.force_single_sentence(response)

    def extract_info_from_messages(self, messages: List[str], patterns: List[str]) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏–π –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º"""
        combined_text = " ".join(messages)
        
        for pattern in patterns:
            match = re.search(pattern, combined_text, re.IGNORECASE)
            if match:
                extracted = match.group(1).strip()
                # –û—á–∏—â–∞–µ–º extracted –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–ª–æ–≤
                cleaned = self.clean_extracted_info(extracted)
                if cleaned:
                    return cleaned
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é —á–∞—Å—Ç—å
        if messages:
            return self.get_most_relevant_part(messages[0])
        
        return ""

    def clean_extracted_info(self, info: str) -> str:
        """–û—á–∏—â–∞–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é"""
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ —Å–ª–æ–≤–∞ –∏ —Ñ—Ä–∞–∑—ã
        stop_phrases = ['—á—Ç–æ', '–∫–æ—Ç–æ—Ä—ã–π', '–∫–æ–≥–¥–∞', '–≥–¥–µ', '–∫–∞–∫', '–æ—á–µ–Ω—å', '–ø—Ä–æ—Å—Ç–æ']
        words = info.split()
        cleaned_words = [word for word in words if word.lower() not in stop_phrases]
        
        cleaned = ' '.join(cleaned_words)
        
        # –û–±—Ä–µ–∑–∞–µ–º –¥–æ —Ä–∞–∑—É–º–Ω–æ–π –¥–ª–∏–Ω—ã
        if len(cleaned) > 50:
            cleaned = cleaned[:50] + "..."
        
        return cleaned

    def get_most_relevant_part(self, text: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é —á–∞—Å—Ç—å —Ç–µ–∫—Å—Ç–∞"""
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ –±–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω–æ–µ
        sentences = re.split(r'[.!?]', text)
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 10 and not any(word in sentence.lower() for word in ['–ø—Ä–∏–≤–µ—Ç', '–ø–æ–∫–∞', '—Å–ø–∞—Å–∏–±–æ']):
                return sentence[:100].strip()
        
        return text[:80].strip()

    def clean_context_response(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        # –£–¥–∞–ª—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ñ—Ä–∞–∑—ã –∏ –ª–∏—à–Ω–µ–µ
        cleanup_patterns = [
            r'–ß–µ–ª–æ–≤–µ–∫:\s*',
            r'–¢—ã:\s*',
            r'Assistant:\s*',
            r'User:\s*',
        ]
        
        cleaned = text
        for pattern in cleanup_patterns:
            cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE)
        
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()
        
        # –û–±—Ä–µ–∑–∞–µ–º –¥–æ —Ä–∞–∑—É–º–Ω–æ–π –¥–ª–∏–Ω—ã
        if len(cleaned) > 120:
            sentences = re.split(r'[.!?]', cleaned)
            if sentences:
                cleaned = sentences[0].strip() + '.'
        
        return cleaned

    def is_good_answer(self, answer: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –æ—Ç–≤–µ—Ç –∞–¥–µ–∫–≤–∞—Ç–Ω—ã–π"""
        if not answer or len(answer) < 3:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –±—Ä–µ–¥
        if self.contains_gibberish(answer):
            return False
        
        bad_patterns = [
            '–Ω–µ –∑–Ω–∞—é', '–Ω–µ —É–∫–∞–∑–∞–Ω–æ', '–Ω–µ —Å–∫–∞–∑–∞–Ω–æ', '–Ω–µ —É–ø–æ–º–∏–Ω–∞–µ—Ç—Å—è',
            '–Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏', '–Ω–µ –º–æ–≥—É', '–Ω–µ –Ω–∞–π–¥–µ–Ω–æ', '...', '??', '!!'
        ]
        
        if any(pattern in answer.lower() for pattern in bad_patterns):
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ—Ç–≤–µ—Ç –Ω–µ —Å–æ—Å—Ç–æ–∏—Ç —Ç–æ–ª—å–∫–æ –∏–∑ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        if re.match(r'^[^\w–∞-—è–ê-–Ø]*$', answer):
            return False
        
        return True

    def fallback_generation(self, message: str) -> str:
        """–û–±—ã—á–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –µ—Å–ª–∏ –ø–æ–∏—Å–∫ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª —Å —Å—Ç—Ä–æ–≥–∏–º –∫–æ–Ω—Ç—Ä–æ–ª–µ–º"""
        try:
            prompt = self.build_smart_prompt(message)
            
            inputs = self.tokenizer.encode(
                prompt, 
                return_tensors="pt", 
                max_length=512,
                truncation=True
            )
            
            attention_mask = torch.ones_like(inputs)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    attention_mask=attention_mask,
                    max_length=len(inputs[0]) + 40,  # –£–∫–æ—Ä–æ—Ç–∏–ª–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É
                    num_return_sequences=1,
                    temperature=0.8,  # –ù–µ–º–Ω–æ–≥–æ —É–≤–µ–ª–∏—á–∏–ª–∏ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.3,  # –£–≤–µ–ª–∏—á–∏–ª–∏ —à—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
                    no_repeat_ngram_size=3,  # –£–≤–µ–ª–∏—á–∏–ª–∏ —Ä–∞–∑–º–µ—Ä n-gram –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π
                    early_stopping=True,
                    top_p=0.9,
                )
            
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            response = response.replace(prompt, "").strip()
            response = self.aggressive_cleaning(response)
            
            return response if response else "–ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ! –†–∞—Å—Å–∫–∞–∂–∏ –ø–æ–¥—Ä–æ–±–Ω–µ–µ."
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            return "–î–∞–≤–∞–π –ø–æ–≥–æ–≤–æ—Ä–∏–º –æ —á–µ–º-–Ω–∏–±—É–¥—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–º!"

    def aggressive_cleaning(self, text: str) -> str:
        """–ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∏ –æ–±—Ä–µ–∑–∫–∞ –¥–æ –ø–µ—Ä–≤–æ–≥–æ –∑–∞–∫–æ–Ω—á–µ–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"""
        if not text:
            return ""
        
        # –£–¥–∞–ª—è–µ–º –≤—Å–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        patterns = [
            r'Assistant\s*:?\s*',
            r'User\s*:?\s*',
            r'–ß–µ–ª–æ–≤–µ–∫\s*:?\s*', 
            r'–¢—ã\s*:?\s*',
            r'–Ø\s+—Å–∫–∞–∑–∞–ª\s+',
            r'–Ø\s+–æ—Ç–≤–µ—Ç–∏–ª\s+',
            r'–Ø\s+—Å–ø—Ä–æ—Å–∏–ª\s+',
            r'–ø–æ—Ç–æ–º\s+—è\s+',
            r'–∑–∞—Ç–µ–º\s+—è\s+',
        ]
        
        cleaned = text
        for pattern in patterns:
            cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE)
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()
        
        # –û–ë–†–ï–ó–ê–ï–ú –î–û –ü–ï–†–í–û–ì–û –ó–ê–ö–û–ù–ß–ï–ù–ù–û–ì–û –ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø
        cleaned = self.force_single_sentence(cleaned)
        
        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –æ—Ç–≤–µ—Ç –Ω–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π
        if len(cleaned) > 120:
            cleaned = cleaned[:120].strip()
            # –ù–∞—Ö–æ–¥–∏–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ –∑–∞–∫–æ–Ω—á–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –≤ –æ–±—Ä–µ–∑–∞–Ω–Ω–æ–º —Ç–µ–∫—Å—Ç–µ
            cleaned = self.force_single_sentence(cleaned)
        
        return cleaned

    def force_single_sentence(self, text: str) -> str:
        """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ–±—Ä–µ–∑–∞–µ—Ç —Ç–µ–∫—Å—Ç –¥–æ –ø–µ—Ä–≤–æ–≥–æ –∑–∞–∫–æ–Ω—á–µ–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"""
        if not text:
            return ""
        
        # –ò—â–µ–º –∫–æ–Ω–µ—Ü –ø–µ—Ä–≤–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentence_endings = ['.', '!', '?', '‚Ä¶']
        
        for i, char in enumerate(text):
            if char in sentence_endings:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∫–æ–Ω–µ—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                if i == len(text) - 1 or text[i+1] in [' ', '\n', '"', '¬ª']:
                    return text[:i+1].strip()
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –∫–æ–Ω–µ—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –¥–æ–±–∞–≤–ª—è–µ–º —Ç–æ—á–∫—É –≤ –ø–æ–¥—Ö–æ–¥—è—â–µ–º –º–µ—Å—Ç–µ
        if len(text) > 80:
            # –û–±—Ä–µ–∑–∞–µ–º –¥–æ 80 —Å–∏–º–≤–æ–ª–æ–≤ –∏ –¥–æ–±–∞–≤–ª—è–µ–º —Ç–æ—á–∫—É
            return text[:80].strip() + '...'
        elif len(text) > 20:
            # –ü—Ä–æ—Å—Ç–æ –¥–æ–±–∞–≤–ª—è–µ–º —Ç–æ—á–∫—É
            return text.strip() + '.'
        else:
            # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç, –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
            return text.strip()

    def clean_response(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞ —Å –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –æ–±—Ä–µ–∑–∫–æ–π"""
        patterns = [
            r'Assistant\s*:?\s*',
            r'User\s*:?\s*',
            r'–ß–µ–ª–æ–≤–µ–∫\s*:?\s*', 
            r'–¢—ã\s*:?\s*',
        ]
        
        cleaned = text
        for pattern in patterns:
            cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE)
        
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()
        
        # –ü–†–ò–ú–ï–ù–Ø–ï–ú –ê–ì–†–ï–°–°–ò–í–ù–£–Æ –û–ë–†–ï–ó–ö–£
        cleaned = self.force_single_sentence(cleaned)
        
        return cleaned

    def build_smart_prompt(self, user_message: str) -> str:
        """–°—Ç—Ä–æ–∏—Ç –ø—Ä–æ–º–ø—Ç —Å –±–∞–∑–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π"""
        personal_info = self.knowledge.get("personal_info", {})
        interests = self.knowledge.get("interests", [])
        habits = self.knowledge.get("habits", [])
        friends = self.knowledge.get("friends", {})
        
        persona_parts = []
        if name := personal_info.get("name"):
            persona_parts.append(f"–ú–µ–Ω—è –∑–æ–≤—É—Ç {name}")
        if age := personal_info.get("age"):
            persona_parts.append(f"–ú–Ω–µ {age} –ª–µ—Ç")
        if city := personal_info.get("city"):
            persona_parts.append(f"–ñ–∏–≤—É –≤ {city}")
        if interests:
            persona_parts.append(f"–ò–Ω—Ç–µ—Ä–µ—Å—ã: {', '.join(interests[:3])}")
        if habits:
            persona_parts.append(f"–ü—Ä–∏–≤—ã—á–∫–∏: {', '.join(habits[:2])}")
        if friends:
            friend_names = list(friends.keys())[:2]
            persona_parts.append(f"–î—Ä—É–∑—å—è: {', '.join(friend_names)}")
        
        persona = ". ".join(persona_parts) if persona_parts else "–Ø AI-–∫–ª–æ–Ω, –æ–±—É—á–µ–Ω–Ω—ã–π –Ω–∞ –≤–∞—à–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏—è—Ö"
        
        prompt = f"""–û—Ç–≤–µ—á–∞–π –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ –∫–∞–∫ —á–µ–ª–æ–≤–µ–∫.
        
{persona}

–ß–µ–ª–æ–≤–µ–∫: {user_message}
–¢—ã:"""
        
        return prompt

    def clean_response(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞"""
        patterns = [
            r'Assistant\s*:?\s*',
            r'User\s*:?\s*',
            r'–ß–µ–ª–æ–≤–µ–∫\s*:?\s*', 
            r'–¢—ã\s*:?\s*',
        ]
        
        cleaned = text
        for pattern in patterns:
            cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE)
        
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()
        return cleaned